{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "숙박예약 사이트인 여기어때에서 \n",
        "평점과 리뷰텍스트를 들고와서 분석하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%python\n",
        "\n",
        "# 필요한 library import 하기\n",
        "%matplotlib inline\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%python\n",
        "# 여기어때에서 크롤링한 평점과 리뷰텍스트 csv 들고오기\n",
        "df = pd.read_csv('hotel_review.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%python\n",
        "\n",
        "# 약 32000개의 data가 있는 것을 알 수 있다.\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%python\n",
        "\n",
        "# 결측치 확인하기\n",
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%python\n",
        "# 텍스트 데이터 전처리하기\n",
        "# 형태소 분석기(konlpy), 정규 표현식(re) 이용\n",
        "\n",
        "\n",
        "from konlpy.tag import Okt\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "def apply_regular_expression(review):\n",
        "    hangul = re.compile('[^ ㄱ-ㅣ 가-힣]')  # 한글 추출 규칙: 띄어 쓰기(1 개)를 포함한 한글\n",
        "    result = hangul.sub('', review)  # 위에 설정한 \"hangul\"규칙을 \"review\"에 적용(.sub)시킴\n",
        "    return result\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%python\n",
        "\n",
        "# 말뭉치 생성\n",
        "corpus = \"\".join(df['review'].tolist())\n",
        "corpus\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%python\n",
        "\n",
        "# 말뭉치를 정규 표현식에 적용\n",
        "apply_regular_expression(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%python\n",
        "\n",
        "# 명사빈도 구하기(최상위 15개 출력해보기)\n",
        "\n",
        "nouns = okt.nouns(apply_regular_expression(corpus))\n",
        "counter = Counter(nouns)\n",
        "counter.most_common(15)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%python\n",
        "\n",
        "# 불용어 사전 load\n",
        "\n",
        "stopwords = pd.read_csv(\"https://raw.githubusercontent.com/yoonkt200/FastCampusDataset/master/korean_stopwords.txt\").values.tolist()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%python\n",
        "# 또, 좀, 방 같은 한 글자는 그냥 제거하기\n",
        "\n",
        "noun_words = Counter({x: counter[x] for x in counter if len(x) > 1})\n",
        "noun_words.most_common(15)\n",
        "\n",
        "add_stopwords = ['남자친구', '방도', '기회', '리뷰', '숙소', '여행','별로','그냥','정도','최고','아주','자주','모두','마음','가성','전체','굿굿','다만','마음','제외','살짝','실망','최악','느낌','보통','호텔','그거','체적','매우','방문','공간','이용','항상','만원','사용','제대로','합리','이틀','짱짱','감사','조금','의사','걱정','서울','애용','다음','실망','최악','느낌','보통','호텔','그거','체적','매우','방문','공간','이용','항상','환경','무엇','비도','이번','모든','의향','정비','이하','이도','여의도','굿굿굿','강주','추천','만족','감동','확인']\n",
        "for word in add_stopwords:\n",
        "    stopwords.append(word)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%python\n",
        "# BOW 벡터 생성하기\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "def text_cleaning(review):\n",
        "    hangul = re.compile('[^ ㄱ-ㅣ 가-힣]')  # 정규 표현식 처리\n",
        "    result = hangul.sub('', review)\n",
        "    okt = Okt()  # 형태소 추출\n",
        "    nouns = okt.nouns(result)\n",
        "    nouns = [x for x in nouns if len(x) > 1]  # 한글자 키워드 제거\n",
        "    nouns = [x for x in nouns if x not in stopwords]  # 불용어 제거\n",
        "    return nouns\n",
        "\n",
        "vect = CountVectorizer(tokenizer = lambda x: text_cleaning(x))\n",
        "bow_vect = vect.fit_transform(df['review'].tolist())\n",
        "word_list = vect.get_feature_names()\n",
        "count_list = bow_vect.toarray().sum(axis=0)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%python\n",
        "\n",
        "# 각 단어가 등장한 횟수\n",
        "count_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%python\n",
        "\n",
        "# 각 단어의 리뷰별 등장 횟수\n",
        "bow_vect.toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%python\n",
        "\n",
        "# 단어별 등장 횟수 출력해보기\n",
        "\n",
        "word_count_dict = dict(zip(word_list, count_list))\n",
        "word_count_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%python\n",
        "# TF-IDF로 변환하기\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "tfidf_vectorizer = TfidfTransformer()\n",
        "tf_idf_vect = tfidf_vectorizer.fit_transform(bow_vect)\n",
        "\n",
        "# 단어 중요도 하나만 출력해보기\n",
        "print(tf_idf_vect[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%python\n",
        "print(tf_idf_vect[0].toarray().shape)\n",
        "print(tf_idf_vect[0].toarray())\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%python\n",
        "invert_index_vectorizer = {v: k for k, v in vect.vocabulary_.items()}\n",
        "print(str(invert_index_vectorizer)[:100]+'...')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%python\n",
        "# 감성분류하기 Logistic Regression\n",
        "\n",
        "\n",
        "# df['review'].hist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%python\n",
        "# 평점\n",
        "# 1~3 리뷰 -> 부정적 \n",
        "# 4~5 -> 긍정적\n",
        "\n",
        "def star_pos_neg(star):\n",
        "    if star > 3:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "    \n",
        "df['y'] = df['star'].apply(lambda x: star_pos_neg(x))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%python\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%python\n",
        "df[\"y\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%python\n",
        "# Training set, Test set 나누기\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x = tf_idf_vect\n",
        "y = df['y']\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state=1)\n",
        "\n",
        "x_train.shape, y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%python\n",
        "x_test.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%python\n",
        "# Logistic Regression 모델 학습하기\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# fit in training set\n",
        "lr = LogisticRegression(random_state = 0)\n",
        "lr.fit(x_train, y_train)\n",
        "\n",
        "# predict in test set\n",
        "y_pred = lr.predict(x_test)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%python\n",
        "# 분류 결과\n",
        "\n",
        "print('accuracy: %.2f' % accuracy_score(y_test, y_pred))\n",
        "print('precision: %.2f' % precision_score(y_test, y_pred))\n",
        "print('recall: %.2f' % recall_score(y_test, y_pred))\n",
        "print('F1: %.2f' % f1_score(y_test, y_pred))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%python\n",
        "# confusion matrix\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "confu = confusion_matrix(y_true = y_test, y_pred = y_pred)\n",
        "\n",
        "plt.figure(figsize=(4, 3))\n",
        "sns.heatmap(confu, annot=True, annot_kws={'size':15}, cmap='OrRd', fmt='.10g')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%python\n",
        "# 클래스 불균형 조정하기\n",
        "df['y'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%python\n",
        "positive_random_idx = df[df['y']==1].sample(1267, random_state=12).index.tolist()\n",
        "negative_random_idx = df[df['y']==0].sample(1267, random_state=12).index.tolist()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%python\n",
        "random_idx = positive_random_idx + negative_random_idx\n",
        "x = tf_idf_vect[random_idx]\n",
        "y = df['y'][random_idx]\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%python\n",
        "x_train.shape, y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%python\n",
        "x_test.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%python\n",
        "# 재학습 \n",
        "lr2 = LogisticRegression(random_state = 0)\n",
        "lr2.fit(x_train, y_train)\n",
        "y_pred = lr2.predict(x_test)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%python\n",
        "# test 결과\n",
        "\n",
        "print('accuracy: %.2f' % accuracy_score(y_test, y_pred))\n",
        "print('precision: %.2f' % precision_score(y_test, y_pred))\n",
        "print('recall: %.2f' % recall_score(y_test, y_pred))\n",
        "print('F1: %.2f' % f1_score(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%python\n",
        "# confusion matrix\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "confu = confusion_matrix(y_true = y_test, y_pred = y_pred)\n",
        "\n",
        "plt.figure(figsize=(4, 3))\n",
        "sns.heatmap(confu, annot=True, annot_kws={'size':15}, cmap='OrRd', fmt='.10g')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# 긍정과 부정 모두 비슷하게 잘 맞춘 것을 확인할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%python\n",
        "\n",
        "# 긍정 부정 키워드 분석\n",
        "lr2.coef_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%python\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.bar(range(len(lr2.coef_[0])), lr2.coef_[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%python\n",
        "# 긍정 부정 키워드 top 10 구하기\n",
        "\n",
        "coef_pos_index = sorted(((value, index) for index, value in enumerate(lr2.coef_[0])), reverse = True)\n",
        "coef_neg_index = sorted(((value, index) for index, value in enumerate(lr2.coef_[0])), reverse = False)\n",
        "\n",
        "invert_index_vectorizer = {v: k for k, v in vect.vocabulary_.items()}\n",
        "invert_index_vectorizer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%python\n",
        "# 긍정 데이터 출력\n",
        "\n",
        "for coef in coef_pos_index[:20]:\n",
        "    print(invert_index_vectorizer[coef[1]], coef[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%python\n",
        "# 부정 데이터 출력\n",
        "\n",
        "for coef in coef_neg_index[:20]:\n",
        "    print(invert_index_vectorizer[coef[1]], coef[0])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": [
        "%python\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    },
    "name": "Big_data_project"
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
